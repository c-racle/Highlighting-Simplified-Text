{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6d9b72",
   "metadata": {},
   "source": [
    "# Run models (merged with LoRA adapters) on testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7da72ef-c95b-4b07-b50d-7ac6936f8142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T19:28:23.204971Z",
     "iopub.status.busy": "2025-11-18T19:28:23.204534Z",
     "iopub.status.idle": "2025-11-18T19:28:23.526128Z",
     "shell.execute_reply": "2025-11-18T19:28:23.523285Z",
     "shell.execute_reply.started": "2025-11-18T19:28:23.204942Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache() \u001b[38;5;66;03m#empty the GPU\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() #empty the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f2ce1-aba1-41e6-b900-3f9eca904f16",
   "metadata": {},
   "source": [
    "## Load and merge adapters with the base LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb353743-e772-42bb-9564-c0778a34e3e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T16:14:18.645391Z",
     "iopub.status.busy": "2025-11-19T16:14:18.644761Z",
     "iopub.status.idle": "2025-11-19T16:18:10.322910Z",
     "shell.execute_reply": "2025-11-19T16:18:10.322076Z",
     "shell.execute_reply.started": "2025-11-19T16:14:18.645332Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cracle/data/conda/envs/training_gemma/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading simplifier adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading highlighter adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading end2end adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All adapters loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -----------------------------\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Base model and LoRA adapter paths\n",
    "base_model_name = \"google/gemma-2-9b-it\"\n",
    "adapter_paths = {\n",
    "    \"simplifier\": \"output_jupyter/simplifier_syn_final_seed42_20251118_195853\",\n",
    "     \"highlighter\": \"output_jupyter/highlighter_syn_final_seed42_20251118_200812\",\n",
    "     \"end2end\": \"output_jupyter/end2end_syn_final_seed42_20251118_201751\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Load models and tokenizers\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for name, path in adapter_paths.items():\n",
    "    print(f\"\\nLoading {name} adapter...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizers[name] = tokenizer\n",
    "\n",
    "    # Load base model in FP16\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        device_map=\"auto\",            # Automatically place layers on GPU\n",
    "        torch_dtype=torch.float16     # Use FP16 for H100\n",
    "    )\n",
    "\n",
    "    # Attach LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, path)\n",
    "    model.eval()\n",
    "    models[name] = model\n",
    "\n",
    "print(\"\\n✅ All adapters loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9bdc7-2521-40d4-a3f1-262db5d4392d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T16:24:38.177316Z",
     "iopub.status.busy": "2025-11-19T16:24:38.176917Z",
     "iopub.status.idle": "2025-11-19T16:24:38.248729Z",
     "shell.execute_reply": "2025-11-19T16:24:38.247397Z",
     "shell.execute_reply.started": "2025-11-19T16:24:38.177284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable LoRA params: 4472832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cracle/data/conda/envs/training_gemma/lib/python3.10/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/cracle/data/conda/envs/training_gemma/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "trainable = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "print(\"Trainable LoRA params:\", trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af7ab9",
   "metadata": {},
   "source": [
    "## Run some tests to see if everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6426106-7e82-48ef-b03f-fd496cae1a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T20:18:50.963123Z",
     "iopub.status.busy": "2025-11-18T20:18:50.962730Z",
     "iopub.status.idle": "2025-11-18T20:19:01.881456Z",
     "shell.execute_reply": "2025-11-18T20:19:01.879647Z",
     "shell.execute_reply.started": "2025-11-18T20:18:50.963090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einfacher Text zur Übersetzung ins Deutsche\n",
      "Viele Leute haben Probleme damit, Dinge im Internet zu lesen. Es ist sehr schwierig! Wir helfen ihnen dabei, alles zu verstehen. Unsere Mitarbeiterinnen und Mitarbeiter sprechen Deutsch fließend und wissen genau, wie man Sachen einfach erklärt. Sie arbeiten daran, dass jeder etwas versteht – egal ob groß oder klein, jung oder alt. \n",
      "Es macht uns Spaß, andere Sprachen zu lernen und sie anderen beizubringen. Wenn du mehr darüber erfahren möchtest, was wir tun, kannst du gerne einen Flyer herunterladen. Du findest ihn hier unten. Dort steht alles drin, was du wissen musst. Und falls du Fragen hast, dann melde dich bei uns. Wir freuen uns darauf, dir weiterzuhelfen!\n",
      "\n",
      "\n",
      "\\nHier ein paar Beispiele dafür, was wir schon gemacht haben:\\n- Broschüren / Flugblätter (für verschiedene Themen)\\n- Erklärungen zu Formularen / Gesetzen\\n- Arbeitsverträge / Mietverträge\\n- Leitlinien / Hausordnung\\n- Webseiten.\\n Jeder Text wurde von einer Gruppe überprüft, um sicherzustellen, dass wirklich jeder ihn versteht. Die Mitglieder dieser Gruppen wurden speziell ausgebildet, um diese Aufgabe auszuführen.  Wenn du eine Organisation bist, die sich darum kümmert, dass ihre Website leicht zugänglich ist, geben wir gern Kurse dazu!\\n Hier noch einmal unser Infoflyer - aber diesmal als PDF Datei. So kannst du ihn direkt ansehen und drucken. Viel Spaß beim Lesen! \\n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# run on one example\n",
    "def generate_simplified_text(model, tokenizer, instruction, input_text, \n",
    "                             max_new_tokens=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generates simplified text using the LoRA-adapted Gemma-2-9B model.\n",
    "    \n",
    "    Args:\n",
    "        model: PeftModel with LoRA adapter loaded\n",
    "        tokenizer: corresponding tokenizer\n",
    "        instruction: string, the instruction for simplification\n",
    "        input_text: string, the original German text\n",
    "        max_new_tokens: int, maximum tokens to generate\n",
    "        temperature: float, controls randomness (0-1)\n",
    "        top_p: float, nucleus sampling\n",
    "\n",
    "    Returns:\n",
    "        Simplified text as string\n",
    "    \"\"\"\n",
    "    # Build prompt matching training format\n",
    "    prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n",
    "\n",
    "    # Tokenize prompt\n",
    "    model_device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=False,#True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2  # discourage repetition\n",
    "        )\n",
    "\n",
    "    # Decode and remove prompt\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Strip the original prompt from the output\n",
    "    simplified_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return simplified_text\n",
    "\n",
    "simplified = generate_simplified_text(\n",
    "    model=models[\"simplifier\"],\n",
    "    tokenizer=tokenizers[\"simplifier\"],\n",
    "    instruction=\"Instruction: Vereinfache und schreibe den folgenden Text neu in leichter Sprache, ohne neue Informationen hinzuzufügen. Verwende kurze Sätze und einfache Wörter. Alle Informationen sollen erhalten bleiben, auch wenn es Fehler oder unvollständige Teile gibt.\",\n",
    "    input_text=\"Texte verständlich machen\\nInformationen für Alle\\n\\nUm am gesellschaftlichen Leben teilhaben zu können, müssen Informationen für alle nutzbar sein. Viele Texte sind schwer verständlich und für TEST Menschen mit Lernschwierigkeiten oder Menschen, die die deutsche Sprache erlernen wollen, nicht geeignet. Das ZFI verfügt über geschulte Fachkräfte und übersetzt Texte in Leichte Sprache.\\n\\nTexte in Leichter Sprache sind für viele Menschen besser zu verstehen.\\nWir übersetzen z. B.:\\n\\n    Informationsbroschüren/Flyer\\n    Erklärungen zu Formularen/Gesetzestexte\\n    Arbeitsverträge/Mietverträge\\n    Leitbilder/Hausordnungen\\n    Internetseiten\\n\\nJeder Text wird von der Zielgruppe auf Verständlichkeit und Lesbarkeit geprüft. Alle Prüfer werden durch das ZFI geschult. Für interessierte Einrichtungen bieten wir Schulungen und Vorträge zu/und in Leichter Sprache an.\\n\\nInfoflyer zum Büro für Leichte Sprache als pdf.\\n\",\n",
    "    #input_text=\"Viele Schulen in Deutschland wollen ab nächstem Jahr mehr digitale Geräte verwenden, um den Unterricht moderner zu machen und die Schüler besser auf die Zukunft vorzubereiten.\",\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63a78d",
   "metadata": {},
   "source": [
    "## Run each merged model on their testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f378ee-82b3-482e-8cf6-4c003f30a014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T20:18:29.886387Z",
     "iopub.status.busy": "2025-11-18T20:18:29.885649Z",
     "iopub.status.idle": "2025-11-18T20:18:29.895571Z",
     "shell.execute_reply": "2025-11-18T20:18:29.893792Z",
     "shell.execute_reply.started": "2025-11-18T20:18:29.886350Z"
    }
   },
   "outputs": [],
   "source": [
    "# run on testsets\n",
    "\n",
    "import json\n",
    "\n",
    "def run_inference_on_jsonl(\n",
    "        input_jsonl_path,\n",
    "        output_jsonl_path,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        instruction,\n",
    "        input_field,\n",
    "        output_field,\n",
    "        max_new_tokens=2048,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Runs the generate_simplified_text function on each line in a JSONL file\n",
    "    and writes the results to a new JSONL file.\n",
    "\n",
    "    Args:\n",
    "        input_jsonl_path: path to the input JSONL test set\n",
    "        output_jsonl_path: where to save the output JSONL\n",
    "        model, tokenizer: your model + tokenizer\n",
    "        instruction: instruction string for generate_simplified_text\n",
    "        text_field: field name in each JSONL line containing the input text\n",
    "        output_field: name of the field to store the model output\n",
    "        max_new_tokens, temperature, top_p: generation parameters\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    with open(input_jsonl_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(output_jsonl_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "        for line in infile:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            item = json.loads(line)\n",
    "\n",
    "            # Extract the input text\n",
    "            input_text = item[input_field]\n",
    "            instruction_text = item[instruction]\n",
    "\n",
    "            # Generate prediction\n",
    "            prediction = generate_simplified_text(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                instruction=instruction,\n",
    "                input_text=input_text,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p\n",
    "            )\n",
    "\n",
    "            # Add prediction to output item\n",
    "            item[output_field] = prediction\n",
    "\n",
    "            # Write updated JSON line\n",
    "            outfile.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            count += 1\n",
    "            if count % 50 == 0:\n",
    "                print(f\"{count} lines processed...\")\n",
    "\n",
    "    print(f\"Done! Saved {count} lines to: {output_jsonl_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca12d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T20:18:34.247719Z",
     "iopub.status.busy": "2025-11-18T20:18:34.246963Z",
     "iopub.status.idle": "2025-11-18T20:18:34.806130Z",
     "shell.execute_reply": "2025-11-18T20:18:34.804707Z",
     "shell.execute_reply.started": "2025-11-18T20:18:34.247642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test datasets loaded!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load your test datasets\n",
    "test_files = {\n",
    "    \"simplifier\": \"data/Accessibility_Seminar/datasets_for_models_synthetic/simplifier_test_final.jsonl\",\n",
    "    \"highlighter\": \"data/Accessibility_Seminar/datasets_for_models_synthetic/highlighter_test_final.jsonl\",\n",
    "    \"end2end\": \"data/Accessibility_Seminar/datasets_for_models_synthetic/end2end_test_final.jsonl\"\n",
    "}\n",
    "\n",
    "#datasets = {name: load_dataset(\"json\", data_files=file)[\"train\"] for name, file in test_files.items()}\n",
    "\n",
    "#run_inference_on_jsonl(test_files[\"simplifier\"])\n",
    "print(\"✅ Test datasets loaded!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9acf9-52fa-42d6-a4c3-9e8920db98a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T19:35:34.850885Z",
     "iopub.status.busy": "2025-11-18T19:35:34.850511Z",
     "iopub.status.idle": "2025-11-18T19:41:05.959665Z",
     "shell.execute_reply": "2025-11-18T19:41:05.955935Z",
     "shell.execute_reply.started": "2025-11-18T19:35:34.850851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Saved 27 lines to: data/Accessibility_Seminar/datasets_for_models_synthetic/simplifier_testset_output_final.jsonl\n"
     ]
    }
   ],
   "source": [
    "#simplifier\n",
    "\n",
    "run_inference_on_jsonl(\n",
    "    input_jsonl_path=test_files[\"simplifier\"],\n",
    "    output_jsonl_path=\"data/Accessibility_Seminar/datasets_for_models_synthetic/simplifier_testset_output_final.jsonl\",\n",
    "    model=models[\"simplifier\"],\n",
    "    tokenizer=tokenizers[\"simplifier\"],\n",
    "    instruction=\"instruction\",\n",
    "    input_field=\"input\",       # adjust if your field name is different\n",
    "    output_field=\"model_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15bd44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T20:06:55.802973Z",
     "iopub.status.busy": "2025-11-18T20:06:55.802498Z",
     "iopub.status.idle": "2025-11-18T20:15:48.713780Z",
     "shell.execute_reply": "2025-11-18T20:15:48.712142Z",
     "shell.execute_reply.started": "2025-11-18T20:06:55.802954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Saved 27 lines to: data/Accessibility_Seminar/datasets_for_models_synthetic/highlighter_testset_output_final.jsonl\n"
     ]
    }
   ],
   "source": [
    "# highlighter\n",
    "\n",
    "\n",
    "run_inference_on_jsonl(\n",
    "    input_jsonl_path=test_files[\"simplifier\"],\n",
    "    output_jsonl_path=\"data/Accessibility_Seminar/datasets_for_models_synthetic/highlighter_testset_output_final.jsonl\",\n",
    "    model=models[\"highlighter\"],\n",
    "    tokenizer=tokenizers[\"highlighter\"],\n",
    "    instruction=\"instruction\",\n",
    "    input_field=\"output\",       # adjust if your field name is different\n",
    "    output_field=\"model_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf3bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T20:19:18.023815Z",
     "iopub.status.busy": "2025-11-18T20:19:18.023134Z",
     "iopub.status.idle": "2025-11-18T20:27:48.713389Z",
     "shell.execute_reply": "2025-11-18T20:27:48.711642Z",
     "shell.execute_reply.started": "2025-11-18T20:19:18.023749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Saved 27 lines to: data/Accessibility_Seminar/datasets_for_models_synthetic/end2end_testset_output_final.jsonl\n"
     ]
    }
   ],
   "source": [
    "#end2end\n",
    "run_inference_on_jsonl(\n",
    "    input_jsonl_path=test_files[\"simplifier\"],\n",
    "    output_jsonl_path=\"data/Accessibility_Seminar/datasets_for_models_synthetic/end2end_testset_output_final.jsonl\",\n",
    "    model=models[\"end2end\"],\n",
    "    tokenizer=tokenizers[\"end2end\"],\n",
    "    instruction=\"instruction\",\n",
    "    input_field=\"input\",       # adjust if your field name is different\n",
    "    output_field=\"model_output\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
