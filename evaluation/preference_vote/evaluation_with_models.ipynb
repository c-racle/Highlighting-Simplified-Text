{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14960d5e",
   "metadata": {},
   "source": [
    "# Evaluations of Gemma and Leo-Mistral models\n",
    "\n",
    "Ask the models which of the outputs they prefer according to criteria specified in prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4971ae0d-89c0-4a13-a5f6-c6a263813e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:44:58.047938Z",
     "iopub.status.busy": "2025-11-19T17:44:58.047757Z",
     "iopub.status.idle": "2025-11-19T17:44:58.051697Z",
     "shell.execute_reply": "2025-11-19T17:44:58.050929Z",
     "shell.execute_reply.started": "2025-11-19T17:44:58.047920Z"
    }
   },
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"<|im_start|>system\n",
    "Du bist ein hilfreicher Assistent, der zwei Texte in einfacher Sprache mit Markdown-Annotationen vergleicht und bewertet.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Du erhältst zwei Texte in einfacher Sprache. Die Texte enthalten:\n",
    "- Markdown-Titel (#, ##, ###)\n",
    "- Markdown-Fettmarkierungen (**) für wichtige Wörter oder Phrasen\n",
    "Ausserdem erhältst du den originalen Text in üblicher Sprache und ohne Annotationen.\n",
    "\n",
    "Deine Aufgabe:\n",
    "1. Bewerte, welcher Text die einfachere Sprache verwendet.\n",
    "2. Bewerte, welcher Text die besseren Titel und wichtigen Wörter hat.\n",
    "3. Bewerte, ob Text 1 dem Original ähnlich ist.\n",
    "4. Bewerte, ob Text 2 dem Original ähnlich ist.\n",
    "3. Bewerte, welchen Text du insgesamt besser findest.\n",
    "\n",
    "Kriterien für die Bewertung:\n",
    "- Sprache:\n",
    "  - Sehr einfaches Vokabular und Grammatik\n",
    "  - Sehr kurze Sätze\n",
    "  - Jeder Satz steht in einer neuen Zeile\n",
    "- Annotationen:\n",
    "  - Titel sind kurz und sinnvoll\n",
    "  - Wichtige Wörter sind korrekt markiert\n",
    "  - Es sind nicht zu viele Wörter hintereinander fett markiert\n",
    "- Gesamte Verständlichkeit für Menschen mit kognitiven Beeinträchtigungen\n",
    "\n",
    "Instruktionen:\n",
    "- Wähle für Sprache einen Text (Text 1 oder Text 2)\n",
    "- Wähle für Annotationen einen Text (Text 1 oder Text 2)\n",
    "- Bewerte, ob Text 1 dem Original ähnlich ist {TRUE oder FALSE}\n",
    "- Bewerte, ob Text 2 dem Original ähnlich ist {TRUE oder FALSE}\n",
    "- Wähle welchen Text du allgemein besser findest (Text 1 oder Text 2)\n",
    "- Deine Ausgabe muss genau dieses Format haben:\n",
    "\n",
    "AUSGABE:\n",
    "Sprache: {Text 1 oder Text 2}\n",
    "Annotationen: {Text 1 oder Text 2}\n",
    "Allgemein: {Text 1 oder Text 2}\n",
    "Text 1 ist dem Original ähnlich: {TRUE oder FALSE}\n",
    "Text 2 ist dem Original ähnlich: {TRUE oder FALSE}\n",
    "\n",
    "Begründung:\n",
    "\n",
    "\n",
    "Hier sind die Texte:\n",
    "\n",
    "Text 1:\n",
    "{Text 1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Text 2:\n",
    "{Text 2}\n",
    "\n",
    "\n",
    "\n",
    "Originaler Text:\n",
    "{original}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53faad89-e010-4ec9-bc15-4d33a7480a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:44:58.596390Z",
     "iopub.status.busy": "2025-11-19T17:44:58.595792Z",
     "iopub.status.idle": "2025-11-19T17:45:17.691756Z",
     "shell.execute_reply": "2025-11-19T17:45:17.689981Z",
     "shell.execute_reply.started": "2025-11-19T17:44:58.596340Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cracle/data/conda/envs/training_gemma/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 47.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "###############################################################################\n",
    "# LOAD MODEL\n",
    "###############################################################################\n",
    "\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_gemma = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" else torch.float32#,\n",
    "    #device_map=\"auto\"\n",
    ").to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c9b14a-08f8-4742-af9a-f69b1b49a1fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:45:17.693037Z",
     "iopub.status.busy": "2025-11-19T17:45:17.692620Z",
     "iopub.status.idle": "2025-11-19T17:45:32.334779Z",
     "shell.execute_reply": "2025-11-19T17:45:32.333075Z",
     "shell.execute_reply.started": "2025-11-19T17:45:17.693022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# -------------------------------\n",
    "# Model Configuration\n",
    "# -------------------------------\n",
    "MODEL_NAME = \"LeoLM/leo-mistral-hessianai-7b-chat\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load Tokenizer\n",
    "# -------------------------------\n",
    "tokenizer_leo = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# -------------------------------\n",
    "# Load Model\n",
    "# -------------------------------\n",
    "model_leo = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    #device_map=\"auto\",  # Automatically place layers on available devices\n",
    "    torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" else torch.float32,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd845dd-123e-4137-b765-b6dd25ec9038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:45:32.337729Z",
     "iopub.status.busy": "2025-11-19T17:45:32.337033Z",
     "iopub.status.idle": "2025-11-19T17:45:32.342794Z",
     "shell.execute_reply": "2025-11-19T17:45:32.342246Z",
     "shell.execute_reply.started": "2025-11-19T17:45:32.337670Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_gemma(end2end_output, pipeline_output, original_text):\n",
    "    try:\n",
    "        prompt = PROMPT.replace(\"{Text 1}\", end2end_output).replace(\"{Text 2}\", pipeline_output).replace(\"{original}\", original_text)\n",
    "        inputs = tokenizer_gemma(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model_gemma.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=2048,\n",
    "                temperature=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        raw = tokenizer_gemma.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        return raw\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error in evaluation:\", e)\n",
    "        #traceback.print_exc()\n",
    "        return None   # Ensure failure never crashes main loop    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ed765-2a69-4edd-8c0c-67ad374d6378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:45:32.343460Z",
     "iopub.status.busy": "2025-11-19T17:45:32.343324Z",
     "iopub.status.idle": "2025-11-19T17:45:33.017093Z",
     "shell.execute_reply": "2025-11-19T17:45:33.015797Z",
     "shell.execute_reply.started": "2025-11-19T17:45:32.343447Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_leo(end2end_output, pipeline_output, original_text):\n",
    "    try:\n",
    "        prompt = PROMPT.replace(\"{Text 1}\", end2end_output).replace(\"{Text 2}\", pipeline_output).replace(\"{original}\", original_text)\n",
    "\n",
    "        inputs = tokenizer_leo(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model_leo.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=4096,\n",
    "                temperature=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        raw = tokenizer_leo.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        return raw\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error in evaluation:\", e)\n",
    "        #traceback.print_exc()\n",
    "        return None   # Ensure failure never crashes main loop    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885726c7-1a01-4ecb-8c13-2712dd93cffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:45:33.017856Z",
     "iopub.status.busy": "2025-11-19T17:45:33.017724Z",
     "iopub.status.idle": "2025-11-19T17:45:34.366698Z",
     "shell.execute_reply": "2025-11-19T17:45:34.365103Z",
     "shell.execute_reply.started": "2025-11-19T17:45:33.017844Z"
    }
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import openpyxl\n",
    "\n",
    "def evaluate(END2END_INPUT_FILE, PIPELINE_INPUT_FILE, OUTPUT_FILE):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with open(END2END_INPUT_FILE, \"r\", encoding=\"utf8\") as f:\n",
    "        lines_end2end = [json.loads(l) for l in f]\n",
    "\n",
    "    with open(PIPELINE_INPUT_FILE, \"r\", encoding=\"utf8\") as f2:\n",
    "        lines_pipeline = [json.loads(l) for l in f2]\n",
    "    print(\"length end2end\", len(lines_end2end))\n",
    "    print(\"length pipeline\", len(lines_pipeline))\n",
    "    \n",
    "    out = open(OUTPUT_FILE, \"w\", encoding=\"utf8\")\n",
    "\n",
    "    failed_evaluation_gemma = []\n",
    "    failed_evaluation_leo = []\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    #for entry in tqdm(lines):\n",
    "    for i in range(len(lines_end2end)):\n",
    "        end2end_output = lines_end2end[i][\"model_output\"]\n",
    "        pipeline_output = lines_pipeline[i][\"model_output\"]\n",
    "        original_text = lines_end2end[i][\"input\"]\n",
    "\n",
    "        prompt = PROMPT.replace(\"{Text 1}\", end2end_output).replace(\"{Text 2}\", pipeline_output).replace(\"{original}\", original_text)\n",
    "\n",
    "\n",
    "\n",
    "        evaluation_gemma = evaluate_gemma(end2end_output, pipeline_output, original_text)\n",
    "        evaluation_leo = evaluate_leo(end2end_output, pipeline_output, original_text)\n",
    "\n",
    "\n",
    "        if evaluation_gemma == None:\n",
    "            failed_evaluation_gemma.append(f\"line {i}\")\n",
    "            continue\n",
    "        if evaluation_leo == None:\n",
    "            failed_evaluation_leo.append(f\"line {i}\")\n",
    "            continue\n",
    "        new_entry = {\n",
    "            \"id\": lines_end2end[i][\"id\"],\n",
    "            \"instruction\":prompt,#entry2.get(\"instruction\", \"\"),\n",
    "            \"original\": original_text,\n",
    "            \"end2end_output\": end2end_output,\n",
    "            \"pipeline_output\": pipeline_output,\n",
    "            \"evaluation_gemma\": evaluation_gemma,\n",
    "            \"evaluation_leo\": evaluation_leo\n",
    "        }\n",
    "        out.write(json.dumps(new_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d576dd-94d3-475a-8b3d-44fec8205ae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:45:34.368251Z",
     "iopub.status.busy": "2025-11-19T17:45:34.367932Z",
     "iopub.status.idle": "2025-11-19T17:49:26.725049Z",
     "shell.execute_reply": "2025-11-19T17:49:26.724002Z",
     "shell.execute_reply.started": "2025-11-19T17:45:34.368221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length end2end 27\n",
      "length pipeline 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "END2END_INPUT_FILE= \"evaluation/end2end_testset_output_final.jsonl\"\n",
    "PIPELINE_INPUT_FILE = \"evaluation/pipeline_testset_output_final.jsonl\"\n",
    "OUTPUT_FILE = \"evaluation/evaluation_all2.jsonl\"\n",
    "evaluate(END2END_INPUT_FILE, PIPELINE_INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440c554b-b6ff-4b0b-90fd-fdc2d43ada3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:49:26.726286Z",
     "iopub.status.busy": "2025-11-19T17:49:26.726065Z",
     "iopub.status.idle": "2025-11-19T17:49:27.567673Z",
     "shell.execute_reply": "2025-11-19T17:49:27.566416Z",
     "shell.execute_reply.started": "2025-11-19T17:49:26.726264Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def extract_after_assistant(text: str):\n",
    "    \"\"\"\n",
    "    Returns the substring of `text` that occurs after the first occurrence\n",
    "    of the word 'assistant'. If not found, returns the original text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    key = \"assistant\"\n",
    "    idx = text.find(key)\n",
    "    if idx == -1:\n",
    "        return text\n",
    "    return text[idx + len(key):].strip()\n",
    "\n",
    "def jsonl_to_excel(jsonl_path: str, excel_path: str):\n",
    "    \"\"\"\n",
    "    Convert a JSONL file to an Excel file and extract only the part\n",
    "    after 'assistant' for evaluation fields.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                obj = json.loads(line)\n",
    "\n",
    "                # Process evaluation fields\n",
    "                if \"evaluation_gemma\" in obj:\n",
    "                    obj[\"evaluation_gemma\"] = extract_after_assistant(obj[\"evaluation_gemma\"])\n",
    "                if \"evaluation_leo\" in obj:\n",
    "                    obj[\"evaluation_leo\"] = extract_after_assistant(obj[\"evaluation_leo\"])\n",
    "\n",
    "                rows.append(obj)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    print(f\"Excel file created at: {excel_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d961d39f-5dbb-42ef-b4f4-3f2dd0315270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:49:27.568800Z",
     "iopub.status.busy": "2025-11-19T17:49:27.568327Z",
     "iopub.status.idle": "2025-11-19T17:49:46.430065Z",
     "shell.execute_reply": "2025-11-19T17:49:46.428779Z",
     "shell.execute_reply.started": "2025-11-19T17:49:27.568779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file created at: evaluation/evaluation_output_all2.xlsx\n"
     ]
    }
   ],
   "source": [
    "jsonl_to_excel(\"evaluation/evaluation_all2.jsonl\", \"evaluation/evaluation_output_all2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548cda4-4e91-4b27-9e58-23cadf01f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5315c-b7a7-44ad-86dd-efdea8c207bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_gemma",
   "language": "python",
   "name": "training_gemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
